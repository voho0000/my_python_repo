{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 Setting and prepare transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Setting some hyperparameters\n",
    "batchSize = 64 # We set the size of the batch.\n",
    "imageSize = 64 # We set the size of the generated images (64x64).\n",
    "\n",
    "# Creating the transformations\n",
    "# We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
    "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 build generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the generator\n",
    "\n",
    "class G(nn.Module): # We introduce a class to define the generator.\n",
    "\n",
    "    def __init__(self): # We introduce the __init__() function that will define the architecture of the generator.\n",
    "        super(G, self).__init__() # We inherit from the nn.Module tools.\n",
    "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), # We start with an inversed convolution.\n",
    "            nn.BatchNorm2d(512), # We normalize all the features along the dimension of the batch.\n",
    "            nn.ReLU(True), # We apply a ReLU rectification to break the linearity.\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.BatchNorm2d(256), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.BatchNorm2d(128), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.BatchNorm2d(64), # We normalize again.\n",
    "            nn.ReLU(True), # We apply another ReLU.\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), # We add another inversed convolution.\n",
    "            nn.Tanh() # We apply a Tanh rectification to break the linearity and stay between -1 and +1.\n",
    "        )\n",
    "\n",
    "    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output containing the generated images.\n",
    "        output = self.main(input) # We forward propagate the signal through the whole neural network of the generator defined by self.main.\n",
    "        return output # We return the output containing the generated images.\n",
    "\n",
    "# Creating the generator\n",
    "netG = G() # We create the generator object.\n",
    "netG.apply(weights_init) # We initialize all the weights of its neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 build discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the discriminator\n",
    "\n",
    "class D(nn.Module): # We introduce a class to define the discriminator.\n",
    "\n",
    "    def __init__(self): # We introduce the __init__() function that will define the architecture of the discriminator.\n",
    "        super(D, self).__init__() # We inherit from the nn.Module tools.\n",
    "        self.main = nn.Sequential( # We create a meta module of a neural network that will contain a sequence of modules (convolutions, full connections, etc.).\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(256), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(512), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.\n",
    "            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, input): # We define the forward function that takes as argument an input that will be fed to the neural network, and that will return the output which will be a value between 0 and 1.\n",
    "        output = self.main(input) # We forward propagate the signal through the whole neural network of the discriminator defined by self.main.\n",
    "        return output.view(-1) # We return the output which will be a value between 0 and 1.\n",
    "\n",
    "# Creating the discriminator\n",
    "netD = D() # We create the discriminator object.\n",
    "netD.apply(weights_init) # We initialize all the weights of its neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the model (DCGANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1 Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # We create a criterion object that will measure the error between the prediction and the target.\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the discriminator.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) # We create the optimizer object of the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782] Loss_D: 0.0035 Loss_G: 34.8897\n",
      "[0/25][1/782] Loss_D: 0.0019 Loss_G: 35.0996\n",
      "[0/25][2/782] Loss_D: 0.0034 Loss_G: 35.1066\n",
      "[0/25][3/782] Loss_D: 0.0046 Loss_G: 35.4700\n",
      "[0/25][4/782] Loss_D: 0.0042 Loss_G: 34.9679\n",
      "[0/25][5/782] Loss_D: 0.0039 Loss_G: 34.8763\n",
      "[0/25][6/782] Loss_D: 0.0058 Loss_G: 34.5741\n",
      "[0/25][7/782] Loss_D: 0.0040 Loss_G: 34.9144\n",
      "[0/25][8/782] Loss_D: 0.0021 Loss_G: 34.7208\n",
      "[0/25][9/782] Loss_D: 0.0123 Loss_G: 34.7685\n",
      "[0/25][10/782] Loss_D: 0.0030 Loss_G: 34.7548\n",
      "[0/25][11/782] Loss_D: 0.0057 Loss_G: 34.8664\n",
      "[0/25][12/782] Loss_D: 0.0034 Loss_G: 34.5940\n",
      "[0/25][13/782] Loss_D: 0.0045 Loss_G: 34.5235\n",
      "[0/25][14/782] Loss_D: 0.0055 Loss_G: 34.3898\n",
      "[0/25][15/782] Loss_D: 0.0030 Loss_G: 34.1745\n",
      "[0/25][16/782] Loss_D: 0.0025 Loss_G: 34.2257\n",
      "[0/25][17/782] Loss_D: 0.0026 Loss_G: 33.7302\n",
      "[0/25][18/782] Loss_D: 0.0034 Loss_G: 34.0246\n",
      "[0/25][19/782] Loss_D: 0.0030 Loss_G: 33.6545\n",
      "[0/25][20/782] Loss_D: 0.0029 Loss_G: 33.5571\n",
      "[0/25][21/782] Loss_D: 0.0049 Loss_G: 33.1286\n",
      "[0/25][22/782] Loss_D: 0.0011 Loss_G: 33.3993\n",
      "[0/25][23/782] Loss_D: 0.0020 Loss_G: 33.1885\n",
      "[0/25][24/782] Loss_D: 0.0031 Loss_G: 32.7716\n",
      "[0/25][25/782] Loss_D: 0.0030 Loss_G: 32.9609\n",
      "[0/25][26/782] Loss_D: 0.0014 Loss_G: 32.7438\n",
      "[0/25][27/782] Loss_D: 0.0021 Loss_G: 32.0218\n",
      "[0/25][28/782] Loss_D: 0.0016 Loss_G: 30.9778\n",
      "[0/25][29/782] Loss_D: 0.0027 Loss_G: 30.6712\n",
      "[0/25][30/782] Loss_D: 0.0019 Loss_G: 29.3723\n",
      "[0/25][31/782] Loss_D: 0.0010 Loss_G: 25.9993\n",
      "[0/25][32/782] Loss_D: 0.0026 Loss_G: 17.1870\n",
      "[0/25][33/782] Loss_D: 0.0083 Loss_G: 5.7547\n",
      "[0/25][34/782] Loss_D: 1.7442 Loss_G: 23.9438\n",
      "[0/25][35/782] Loss_D: 0.4074 Loss_G: 26.8865\n",
      "[0/25][36/782] Loss_D: 1.4003 Loss_G: 23.3400\n",
      "[0/25][37/782] Loss_D: 0.1895 Loss_G: 17.7525\n",
      "[0/25][38/782] Loss_D: 0.1515 Loss_G: 11.4821\n",
      "[0/25][39/782] Loss_D: 0.0259 Loss_G: 4.7250\n",
      "[0/25][40/782] Loss_D: 0.6203 Loss_G: 12.1979\n",
      "[0/25][41/782] Loss_D: 0.0868 Loss_G: 12.9306\n",
      "[0/25][42/782] Loss_D: 0.2563 Loss_G: 9.0889\n",
      "[0/25][43/782] Loss_D: 0.2454 Loss_G: 5.1696\n",
      "[0/25][44/782] Loss_D: 0.5417 Loss_G: 10.0422\n",
      "[0/25][45/782] Loss_D: 0.3857 Loss_G: 8.5071\n",
      "[0/25][46/782] Loss_D: 0.2885 Loss_G: 4.5547\n",
      "[0/25][47/782] Loss_D: 0.3009 Loss_G: 5.8503\n",
      "[0/25][48/782] Loss_D: 0.0900 Loss_G: 6.4225\n",
      "[0/25][49/782] Loss_D: 0.1383 Loss_G: 5.0231\n",
      "[0/25][50/782] Loss_D: 0.4831 Loss_G: 4.0308\n",
      "[0/25][51/782] Loss_D: 0.3924 Loss_G: 6.6239\n",
      "[0/25][52/782] Loss_D: 0.6125 Loss_G: 3.5345\n",
      "[0/25][53/782] Loss_D: 0.7787 Loss_G: 8.8975\n",
      "[0/25][54/782] Loss_D: 0.8639 Loss_G: 6.7679\n",
      "[0/25][55/782] Loss_D: 0.1664 Loss_G: 3.9288\n",
      "[0/25][56/782] Loss_D: 0.5064 Loss_G: 4.9816\n",
      "[0/25][57/782] Loss_D: 0.2662 Loss_G: 5.2778\n",
      "[0/25][58/782] Loss_D: 0.3206 Loss_G: 4.8391\n",
      "[0/25][59/782] Loss_D: 0.2603 Loss_G: 4.9299\n",
      "[0/25][60/782] Loss_D: 0.2712 Loss_G: 4.9675\n",
      "[0/25][61/782] Loss_D: 0.2502 Loss_G: 4.5385\n",
      "[0/25][62/782] Loss_D: 0.4533 Loss_G: 4.4688\n",
      "[0/25][63/782] Loss_D: 0.3038 Loss_G: 4.7782\n",
      "[0/25][64/782] Loss_D: 0.3347 Loss_G: 4.5111\n",
      "[0/25][65/782] Loss_D: 0.4062 Loss_G: 5.1985\n",
      "[0/25][66/782] Loss_D: 0.3892 Loss_G: 3.1370\n",
      "[0/25][67/782] Loss_D: 0.6326 Loss_G: 9.6581\n",
      "[0/25][68/782] Loss_D: 0.8418 Loss_G: 5.2380\n",
      "[0/25][69/782] Loss_D: 0.1662 Loss_G: 2.7327\n",
      "[0/25][70/782] Loss_D: 0.7310 Loss_G: 7.7925\n",
      "[0/25][71/782] Loss_D: 1.2684 Loss_G: 3.9229\n",
      "[0/25][72/782] Loss_D: 0.3567 Loss_G: 5.2589\n",
      "[0/25][73/782] Loss_D: 0.4508 Loss_G: 5.6201\n",
      "[0/25][74/782] Loss_D: 0.3322 Loss_G: 3.2145\n",
      "[0/25][75/782] Loss_D: 0.3479 Loss_G: 5.8259\n",
      "[0/25][76/782] Loss_D: 0.1835 Loss_G: 5.3726\n",
      "[0/25][77/782] Loss_D: 0.5802 Loss_G: 1.6740\n",
      "[0/25][78/782] Loss_D: 1.2629 Loss_G: 11.9119\n",
      "[0/25][79/782] Loss_D: 3.1310 Loss_G: 8.0747\n",
      "[0/25][80/782] Loss_D: 0.9013 Loss_G: 3.1671\n",
      "[0/25][81/782] Loss_D: 0.2709 Loss_G: 2.3652\n",
      "[0/25][82/782] Loss_D: 0.6848 Loss_G: 5.5456\n",
      "[0/25][83/782] Loss_D: 0.3160 Loss_G: 5.1363\n",
      "[0/25][84/782] Loss_D: 0.3795 Loss_G: 3.2760\n",
      "[0/25][85/782] Loss_D: 0.4086 Loss_G: 4.0893\n",
      "[0/25][86/782] Loss_D: 0.2992 Loss_G: 4.1670\n",
      "[0/25][87/782] Loss_D: 0.3020 Loss_G: 3.9688\n",
      "[0/25][88/782] Loss_D: 0.4765 Loss_G: 2.9928\n",
      "[0/25][89/782] Loss_D: 0.4288 Loss_G: 5.4267\n",
      "[0/25][90/782] Loss_D: 0.4809 Loss_G: 3.3033\n",
      "[0/25][91/782] Loss_D: 0.3169 Loss_G: 4.7310\n",
      "[0/25][92/782] Loss_D: 0.2892 Loss_G: 3.8404\n",
      "[0/25][93/782] Loss_D: 0.4800 Loss_G: 4.8883\n",
      "[0/25][94/782] Loss_D: 0.6351 Loss_G: 3.4248\n",
      "[0/25][95/782] Loss_D: 0.5294 Loss_G: 6.4713\n",
      "[0/25][96/782] Loss_D: 0.6020 Loss_G: 3.2200\n",
      "[0/25][97/782] Loss_D: 0.6837 Loss_G: 6.5025\n",
      "[0/25][98/782] Loss_D: 0.5713 Loss_G: 4.7175\n",
      "[0/25][99/782] Loss_D: 0.5754 Loss_G: 6.1449\n",
      "[0/25][100/782] Loss_D: 0.3252 Loss_G: 4.5462\n",
      "[0/25][101/782] Loss_D: 0.5707 Loss_G: 6.1059\n",
      "[0/25][102/782] Loss_D: 0.3724 Loss_G: 4.4107\n",
      "[0/25][103/782] Loss_D: 0.5083 Loss_G: 6.6475\n",
      "[0/25][104/782] Loss_D: 0.3760 Loss_G: 4.3175\n",
      "[0/25][105/782] Loss_D: 0.3409 Loss_G: 5.4617\n",
      "[0/25][106/782] Loss_D: 0.2986 Loss_G: 5.0191\n",
      "[0/25][107/782] Loss_D: 0.4220 Loss_G: 5.4600\n",
      "[0/25][108/782] Loss_D: 0.7769 Loss_G: 2.1298\n",
      "[0/25][109/782] Loss_D: 1.8831 Loss_G: 9.3808\n",
      "[0/25][110/782] Loss_D: 1.8438 Loss_G: 5.7797\n",
      "[0/25][111/782] Loss_D: 0.3500 Loss_G: 3.2140\n",
      "[0/25][112/782] Loss_D: 0.8269 Loss_G: 9.2344\n",
      "[0/25][113/782] Loss_D: 2.4300 Loss_G: 2.6769\n",
      "[0/25][114/782] Loss_D: 0.8171 Loss_G: 6.3994\n",
      "[0/25][115/782] Loss_D: 0.7215 Loss_G: 4.1173\n",
      "[0/25][116/782] Loss_D: 0.4675 Loss_G: 4.3887\n",
      "[0/25][117/782] Loss_D: 0.6569 Loss_G: 5.7196\n",
      "[0/25][118/782] Loss_D: 0.5636 Loss_G: 3.4293\n",
      "[0/25][119/782] Loss_D: 0.5945 Loss_G: 5.1837\n",
      "[0/25][120/782] Loss_D: 0.2872 Loss_G: 5.2313\n",
      "[0/25][121/782] Loss_D: 0.3050 Loss_G: 4.0061\n",
      "[0/25][122/782] Loss_D: 0.3691 Loss_G: 5.3358\n",
      "[0/25][123/782] Loss_D: 0.3684 Loss_G: 3.9178\n",
      "[0/25][124/782] Loss_D: 0.3275 Loss_G: 5.4961\n",
      "[0/25][125/782] Loss_D: 0.4253 Loss_G: 3.9496\n",
      "[0/25][126/782] Loss_D: 0.6001 Loss_G: 7.6894\n",
      "[0/25][127/782] Loss_D: 0.8765 Loss_G: 2.7423\n",
      "[0/25][128/782] Loss_D: 1.0526 Loss_G: 9.1543\n",
      "[0/25][129/782] Loss_D: 0.7691 Loss_G: 7.2505\n",
      "[0/25][130/782] Loss_D: 0.2270 Loss_G: 3.8132\n",
      "[0/25][131/782] Loss_D: 0.7279 Loss_G: 7.5641\n",
      "[0/25][132/782] Loss_D: 0.7103 Loss_G: 4.4545\n",
      "[0/25][133/782] Loss_D: 0.3390 Loss_G: 5.0742\n",
      "[0/25][134/782] Loss_D: 0.3273 Loss_G: 5.5069\n",
      "[0/25][135/782] Loss_D: 0.3014 Loss_G: 3.8980\n",
      "[0/25][136/782] Loss_D: 0.4901 Loss_G: 6.3701\n",
      "[0/25][137/782] Loss_D: 0.3849 Loss_G: 4.1047\n",
      "[0/25][138/782] Loss_D: 0.3692 Loss_G: 5.7683\n",
      "[0/25][139/782] Loss_D: 0.2186 Loss_G: 5.1191\n",
      "[0/25][140/782] Loss_D: 0.3374 Loss_G: 4.0471\n",
      "[0/25][141/782] Loss_D: 0.4391 Loss_G: 7.2925\n",
      "[0/25][142/782] Loss_D: 0.3962 Loss_G: 4.6789\n",
      "[0/25][143/782] Loss_D: 0.3428 Loss_G: 5.0204\n",
      "[0/25][144/782] Loss_D: 0.3236 Loss_G: 5.5148\n",
      "[0/25][145/782] Loss_D: 0.2365 Loss_G: 5.3159\n",
      "[0/25][146/782] Loss_D: 0.2376 Loss_G: 5.9628\n",
      "[0/25][147/782] Loss_D: 0.2416 Loss_G: 5.1592\n",
      "[0/25][148/782] Loss_D: 0.3009 Loss_G: 5.9420\n",
      "[0/25][149/782] Loss_D: 0.2186 Loss_G: 5.5105\n",
      "[0/25][150/782] Loss_D: 0.3399 Loss_G: 5.6988\n",
      "[0/25][151/782] Loss_D: 0.3015 Loss_G: 5.2746\n",
      "[0/25][152/782] Loss_D: 0.2180 Loss_G: 7.5172\n",
      "[0/25][153/782] Loss_D: 0.2299 Loss_G: 4.6489\n",
      "[0/25][154/782] Loss_D: 0.3579 Loss_G: 6.0211\n",
      "[0/25][155/782] Loss_D: 0.1566 Loss_G: 6.1585\n",
      "[0/25][156/782] Loss_D: 0.1808 Loss_G: 5.2444\n",
      "[0/25][157/782] Loss_D: 0.2618 Loss_G: 5.7506\n",
      "[0/25][158/782] Loss_D: 0.1897 Loss_G: 5.7527\n",
      "[0/25][159/782] Loss_D: 0.3328 Loss_G: 3.7869\n",
      "[0/25][160/782] Loss_D: 0.5178 Loss_G: 13.2387\n",
      "[0/25][161/782] Loss_D: 1.8330 Loss_G: 6.0572\n",
      "[0/25][162/782] Loss_D: 0.3581 Loss_G: 8.8889\n",
      "[0/25][163/782] Loss_D: 1.7253 Loss_G: 1.8968\n",
      "[0/25][164/782] Loss_D: 1.5461 Loss_G: 11.7705\n",
      "[0/25][165/782] Loss_D: 1.1886 Loss_G: 8.0972\n",
      "[0/25][166/782] Loss_D: 0.2633 Loss_G: 4.1382\n",
      "[0/25][167/782] Loss_D: 0.9038 Loss_G: 8.9459\n",
      "[0/25][168/782] Loss_D: 0.5014 Loss_G: 7.0704\n",
      "[0/25][169/782] Loss_D: 0.2374 Loss_G: 4.4380\n",
      "[0/25][170/782] Loss_D: 0.3140 Loss_G: 5.7800\n",
      "[0/25][171/782] Loss_D: 0.1813 Loss_G: 6.0945\n",
      "[0/25][172/782] Loss_D: 0.2743 Loss_G: 4.5873\n",
      "[0/25][173/782] Loss_D: 0.3117 Loss_G: 7.6719\n",
      "[0/25][174/782] Loss_D: 0.2198 Loss_G: 6.4747\n",
      "[0/25][175/782] Loss_D: 0.1863 Loss_G: 4.1448\n",
      "[0/25][176/782] Loss_D: 0.4836 Loss_G: 9.0046\n",
      "[0/25][177/782] Loss_D: 0.3126 Loss_G: 7.7593\n",
      "[0/25][178/782] Loss_D: 0.0795 Loss_G: 5.4976\n",
      "[0/25][179/782] Loss_D: 0.1147 Loss_G: 4.9916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][180/782] Loss_D: 0.1821 Loss_G: 6.9652\n",
      "[0/25][181/782] Loss_D: 0.1259 Loss_G: 5.9860\n",
      "[0/25][182/782] Loss_D: 0.1246 Loss_G: 4.8215\n",
      "[0/25][183/782] Loss_D: 0.1688 Loss_G: 6.5439\n",
      "[0/25][184/782] Loss_D: 0.2295 Loss_G: 4.9326\n",
      "[0/25][185/782] Loss_D: 0.1443 Loss_G: 5.8251\n",
      "[0/25][186/782] Loss_D: 0.1943 Loss_G: 5.9231\n",
      "[0/25][187/782] Loss_D: 0.1189 Loss_G: 5.7636\n",
      "[0/25][188/782] Loss_D: 0.1510 Loss_G: 6.1988\n",
      "[0/25][189/782] Loss_D: 0.1848 Loss_G: 6.5610\n",
      "[0/25][190/782] Loss_D: 0.1516 Loss_G: 7.0023\n",
      "[0/25][191/782] Loss_D: 0.1643 Loss_G: 6.5094\n",
      "[0/25][192/782] Loss_D: 0.3877 Loss_G: 8.0236\n",
      "[0/25][193/782] Loss_D: 0.1110 Loss_G: 7.2601\n",
      "[0/25][194/782] Loss_D: 0.2742 Loss_G: 5.7898\n",
      "[0/25][195/782] Loss_D: 0.3031 Loss_G: 7.5616\n",
      "[0/25][196/782] Loss_D: 0.1772 Loss_G: 6.2356\n",
      "[0/25][197/782] Loss_D: 0.1888 Loss_G: 7.9196\n",
      "[0/25][198/782] Loss_D: 0.1273 Loss_G: 6.4003\n",
      "[0/25][199/782] Loss_D: 0.1334 Loss_G: 7.4749\n",
      "[0/25][200/782] Loss_D: 0.2360 Loss_G: 5.5075\n",
      "[0/25][201/782] Loss_D: 0.1840 Loss_G: 8.2554\n",
      "[0/25][202/782] Loss_D: 0.0973 Loss_G: 7.4036\n",
      "[0/25][203/782] Loss_D: 0.1098 Loss_G: 6.3376\n",
      "[0/25][204/782] Loss_D: 0.3172 Loss_G: 9.0321\n",
      "[0/25][205/782] Loss_D: 0.3065 Loss_G: 6.7065\n",
      "[0/25][206/782] Loss_D: 0.1684 Loss_G: 8.6522\n",
      "[0/25][207/782] Loss_D: 0.0525 Loss_G: 7.6274\n",
      "[0/25][208/782] Loss_D: 0.1390 Loss_G: 8.5800\n",
      "[0/25][209/782] Loss_D: 0.1804 Loss_G: 6.6175\n",
      "[0/25][210/782] Loss_D: 0.5742 Loss_G: 18.6731\n",
      "[0/25][211/782] Loss_D: 1.6157 Loss_G: 12.3945\n",
      "[0/25][212/782] Loss_D: 0.0707 Loss_G: 6.2932\n",
      "[0/25][213/782] Loss_D: 0.7277 Loss_G: 17.9398\n",
      "[0/25][214/782] Loss_D: 0.7531 Loss_G: 17.1669\n",
      "[0/25][215/782] Loss_D: 0.0357 Loss_G: 14.6418\n",
      "[0/25][216/782] Loss_D: 0.0129 Loss_G: 11.6062\n",
      "[0/25][217/782] Loss_D: 0.0163 Loss_G: 8.1598\n",
      "[0/25][218/782] Loss_D: 0.2186 Loss_G: 10.3079\n",
      "[0/25][219/782] Loss_D: 0.0280 Loss_G: 9.5038\n",
      "[0/25][220/782] Loss_D: 0.2463 Loss_G: 5.5706\n",
      "[0/25][221/782] Loss_D: 0.4699 Loss_G: 15.3337\n",
      "[0/25][222/782] Loss_D: 0.4250 Loss_G: 14.7312\n",
      "[0/25][223/782] Loss_D: 0.0435 Loss_G: 11.8212\n",
      "[0/25][224/782] Loss_D: 0.0362 Loss_G: 7.7061\n",
      "[0/25][225/782] Loss_D: 0.0802 Loss_G: 5.5172\n",
      "[0/25][226/782] Loss_D: 0.4149 Loss_G: 13.9909\n",
      "[0/25][227/782] Loss_D: 0.1411 Loss_G: 14.8028\n",
      "[0/25][228/782] Loss_D: 0.1870 Loss_G: 11.5290\n",
      "[0/25][229/782] Loss_D: 0.0707 Loss_G: 7.2204\n",
      "[0/25][230/782] Loss_D: 0.2409 Loss_G: 10.2880\n",
      "[0/25][231/782] Loss_D: 0.2252 Loss_G: 8.1299\n",
      "[0/25][232/782] Loss_D: 0.2723 Loss_G: 8.5786\n",
      "[0/25][233/782] Loss_D: 0.1601 Loss_G: 6.9379\n",
      "[0/25][234/782] Loss_D: 0.3364 Loss_G: 11.1372\n",
      "[0/25][235/782] Loss_D: 0.3888 Loss_G: 7.3205\n",
      "[0/25][236/782] Loss_D: 0.4128 Loss_G: 4.9775\n",
      "[0/25][237/782] Loss_D: 1.1140 Loss_G: 17.2424\n",
      "[0/25][238/782] Loss_D: 5.8396 Loss_G: 11.6818\n",
      "[0/25][239/782] Loss_D: 0.5869 Loss_G: 5.5213\n",
      "[0/25][240/782] Loss_D: 0.9129 Loss_G: 7.1472\n",
      "[0/25][241/782] Loss_D: 0.5078 Loss_G: 9.5928\n",
      "[0/25][242/782] Loss_D: 0.4066 Loss_G: 6.1684\n",
      "[0/25][243/782] Loss_D: 0.9112 Loss_G: 9.0534\n",
      "[0/25][244/782] Loss_D: 0.9071 Loss_G: 5.4531\n",
      "[0/25][245/782] Loss_D: 0.4590 Loss_G: 4.4280\n",
      "[0/25][246/782] Loss_D: 0.8195 Loss_G: 7.9630\n",
      "[0/25][247/782] Loss_D: 1.4700 Loss_G: 2.5044\n",
      "[0/25][248/782] Loss_D: 1.3932 Loss_G: 8.9197\n",
      "[0/25][249/782] Loss_D: 0.9938 Loss_G: 6.1785\n",
      "[0/25][250/782] Loss_D: 0.1724 Loss_G: 4.0510\n",
      "[0/25][251/782] Loss_D: 0.3471 Loss_G: 5.8516\n",
      "[0/25][252/782] Loss_D: 0.2753 Loss_G: 4.6351\n",
      "[0/25][253/782] Loss_D: 0.1449 Loss_G: 4.2896\n",
      "[0/25][254/782] Loss_D: 0.3405 Loss_G: 3.8649\n",
      "[0/25][255/782] Loss_D: 0.2451 Loss_G: 3.8878\n",
      "[0/25][256/782] Loss_D: 0.1336 Loss_G: 4.8145\n",
      "[0/25][257/782] Loss_D: 0.1786 Loss_G: 4.0253\n",
      "[0/25][258/782] Loss_D: 0.1869 Loss_G: 4.2849\n",
      "[0/25][259/782] Loss_D: 0.1811 Loss_G: 4.3920\n",
      "[0/25][260/782] Loss_D: 0.2268 Loss_G: 3.1935\n",
      "[0/25][261/782] Loss_D: 0.3080 Loss_G: 6.8440\n",
      "[0/25][262/782] Loss_D: 0.3238 Loss_G: 4.6473\n",
      "[0/25][263/782] Loss_D: 0.1161 Loss_G: 3.5964\n",
      "[0/25][264/782] Loss_D: 0.3130 Loss_G: 7.2922\n",
      "[0/25][265/782] Loss_D: 0.3447 Loss_G: 5.2600\n",
      "[0/25][266/782] Loss_D: 0.2365 Loss_G: 4.1237\n",
      "[0/25][267/782] Loss_D: 0.3626 Loss_G: 9.0365\n",
      "[0/25][268/782] Loss_D: 0.4126 Loss_G: 6.6073\n",
      "[0/25][269/782] Loss_D: 0.2255 Loss_G: 3.8439\n",
      "[0/25][270/782] Loss_D: 0.4954 Loss_G: 10.6221\n",
      "[0/25][271/782] Loss_D: 0.6361 Loss_G: 9.0312\n",
      "[0/25][272/782] Loss_D: 0.1729 Loss_G: 5.2431\n",
      "[0/25][273/782] Loss_D: 0.4948 Loss_G: 8.0012\n",
      "[0/25][274/782] Loss_D: 0.0940 Loss_G: 7.5248\n",
      "[0/25][275/782] Loss_D: 0.1176 Loss_G: 5.7713\n",
      "[0/25][276/782] Loss_D: 0.1231 Loss_G: 4.2996\n",
      "[0/25][277/782] Loss_D: 0.2637 Loss_G: 6.9104\n",
      "[0/25][278/782] Loss_D: 0.1624 Loss_G: 6.3436\n",
      "[0/25][279/782] Loss_D: 0.0852 Loss_G: 4.7932\n",
      "[0/25][280/782] Loss_D: 0.2349 Loss_G: 4.7795\n",
      "[0/25][281/782] Loss_D: 0.3623 Loss_G: 5.3926\n",
      "[0/25][282/782] Loss_D: 0.1273 Loss_G: 5.8176\n",
      "[0/25][283/782] Loss_D: 0.2133 Loss_G: 5.4149\n",
      "[0/25][284/782] Loss_D: 0.3180 Loss_G: 4.9117\n",
      "[0/25][285/782] Loss_D: 0.5412 Loss_G: 3.3934\n",
      "[0/25][286/782] Loss_D: 0.4467 Loss_G: 8.2336\n",
      "[0/25][287/782] Loss_D: 0.4679 Loss_G: 5.3579\n",
      "[0/25][288/782] Loss_D: 0.3376 Loss_G: 7.1476\n",
      "[0/25][289/782] Loss_D: 0.2247 Loss_G: 5.3904\n",
      "[0/25][290/782] Loss_D: 0.0848 Loss_G: 4.4038\n",
      "[0/25][291/782] Loss_D: 0.7804 Loss_G: 9.4923\n",
      "[0/25][292/782] Loss_D: 0.6167 Loss_G: 6.8563\n",
      "[0/25][293/782] Loss_D: 0.3537 Loss_G: 3.4904\n",
      "[0/25][294/782] Loss_D: 0.5598 Loss_G: 9.0159\n",
      "[0/25][295/782] Loss_D: 0.5862 Loss_G: 6.6365\n",
      "[0/25][296/782] Loss_D: 0.1604 Loss_G: 4.5290\n",
      "[0/25][297/782] Loss_D: 0.2333 Loss_G: 5.0461\n",
      "[0/25][298/782] Loss_D: 0.1671 Loss_G: 5.2387\n",
      "[0/25][299/782] Loss_D: 0.1047 Loss_G: 4.8479\n",
      "[0/25][300/782] Loss_D: 0.2837 Loss_G: 3.6697\n",
      "[0/25][301/782] Loss_D: 0.2585 Loss_G: 5.1137\n",
      "[0/25][302/782] Loss_D: 0.2656 Loss_G: 4.1974\n",
      "[0/25][303/782] Loss_D: 0.3484 Loss_G: 4.1606\n",
      "[0/25][304/782] Loss_D: 0.4600 Loss_G: 3.9213\n",
      "[0/25][305/782] Loss_D: 0.3590 Loss_G: 4.1384\n",
      "[0/25][306/782] Loss_D: 0.3334 Loss_G: 4.7030\n",
      "[0/25][307/782] Loss_D: 0.2405 Loss_G: 4.7061\n",
      "[0/25][308/782] Loss_D: 0.3240 Loss_G: 3.9285\n",
      "[0/25][309/782] Loss_D: 0.2983 Loss_G: 5.6258\n",
      "[0/25][310/782] Loss_D: 0.2002 Loss_G: 4.4509\n",
      "[0/25][311/782] Loss_D: 0.1515 Loss_G: 4.7612\n",
      "[0/25][312/782] Loss_D: 0.2634 Loss_G: 4.9136\n",
      "[0/25][313/782] Loss_D: 0.4029 Loss_G: 3.0290\n",
      "[0/25][314/782] Loss_D: 0.5014 Loss_G: 7.0556\n",
      "[0/25][315/782] Loss_D: 1.6872 Loss_G: 0.0911\n",
      "[0/25][316/782] Loss_D: 3.6090 Loss_G: 11.0950\n",
      "[0/25][317/782] Loss_D: 3.6594 Loss_G: 6.2399\n",
      "[0/25][318/782] Loss_D: 0.3755 Loss_G: 2.7691\n",
      "[0/25][319/782] Loss_D: 0.5267 Loss_G: 2.9262\n",
      "[0/25][320/782] Loss_D: 0.2704 Loss_G: 3.9107\n",
      "[0/25][321/782] Loss_D: 0.3781 Loss_G: 3.3335\n",
      "[0/25][322/782] Loss_D: 0.2252 Loss_G: 3.5927\n",
      "[0/25][323/782] Loss_D: 0.2426 Loss_G: 3.4984\n",
      "[0/25][324/782] Loss_D: 0.2251 Loss_G: 3.7029\n",
      "[0/25][325/782] Loss_D: 0.2487 Loss_G: 3.4823\n",
      "[0/25][326/782] Loss_D: 0.2562 Loss_G: 3.6885\n",
      "[0/25][327/782] Loss_D: 0.3918 Loss_G: 2.8528\n",
      "[0/25][328/782] Loss_D: 0.2696 Loss_G: 4.3099\n",
      "[0/25][329/782] Loss_D: 0.1102 Loss_G: 4.6994\n",
      "[0/25][330/782] Loss_D: 0.1801 Loss_G: 3.9134\n",
      "[0/25][331/782] Loss_D: 0.1964 Loss_G: 3.8701\n",
      "[0/25][332/782] Loss_D: 0.1924 Loss_G: 4.5088\n",
      "[0/25][333/782] Loss_D: 0.2046 Loss_G: 4.0454\n",
      "[0/25][334/782] Loss_D: 0.3540 Loss_G: 3.8094\n",
      "[0/25][335/782] Loss_D: 0.1402 Loss_G: 4.4432\n",
      "[0/25][336/782] Loss_D: 0.2655 Loss_G: 3.5131\n",
      "[0/25][337/782] Loss_D: 0.1838 Loss_G: 4.5086\n",
      "[0/25][338/782] Loss_D: 0.1916 Loss_G: 3.7462\n",
      "[0/25][339/782] Loss_D: 0.1706 Loss_G: 3.9470\n",
      "[0/25][340/782] Loss_D: 0.2443 Loss_G: 3.6050\n",
      "[0/25][341/782] Loss_D: 0.2527 Loss_G: 5.3311\n",
      "[0/25][342/782] Loss_D: 0.2334 Loss_G: 4.1317\n",
      "[0/25][343/782] Loss_D: 0.1584 Loss_G: 3.4173\n",
      "[0/25][344/782] Loss_D: 0.3963 Loss_G: 4.7924\n",
      "[0/25][345/782] Loss_D: 0.2593 Loss_G: 3.8577\n",
      "[0/25][346/782] Loss_D: 0.3731 Loss_G: 2.8661\n",
      "[0/25][347/782] Loss_D: 0.4371 Loss_G: 9.1966\n",
      "[0/25][348/782] Loss_D: 0.9550 Loss_G: 6.0969\n",
      "[0/25][349/782] Loss_D: 0.2157 Loss_G: 3.8559\n",
      "[0/25][350/782] Loss_D: 0.2400 Loss_G: 4.5678\n",
      "[0/25][351/782] Loss_D: 0.1290 Loss_G: 4.9310\n",
      "[0/25][352/782] Loss_D: 0.0943 Loss_G: 5.1577\n",
      "[0/25][353/782] Loss_D: 0.3260 Loss_G: 4.6667\n",
      "[0/25][354/782] Loss_D: 0.6794 Loss_G: 6.8535\n",
      "[0/25][355/782] Loss_D: 1.5874 Loss_G: 1.9871\n",
      "[0/25][356/782] Loss_D: 2.4857 Loss_G: 12.0205\n",
      "[0/25][357/782] Loss_D: 1.9586 Loss_G: 9.3956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][358/782] Loss_D: 0.5556 Loss_G: 4.3824\n",
      "[0/25][359/782] Loss_D: 2.0417 Loss_G: 9.6066\n",
      "[0/25][360/782] Loss_D: 0.9149 Loss_G: 7.4864\n",
      "[0/25][361/782] Loss_D: 0.3580 Loss_G: 4.9417\n",
      "[0/25][362/782] Loss_D: 0.6368 Loss_G: 6.3901\n",
      "[0/25][363/782] Loss_D: 0.4609 Loss_G: 5.8274\n",
      "[0/25][364/782] Loss_D: 0.8825 Loss_G: 4.7083\n",
      "[0/25][365/782] Loss_D: 1.0049 Loss_G: 6.4771\n",
      "[0/25][366/782] Loss_D: 0.8574 Loss_G: 4.1116\n",
      "[0/25][367/782] Loss_D: 1.0411 Loss_G: 6.5740\n",
      "[0/25][368/782] Loss_D: 0.2414 Loss_G: 6.2645\n",
      "[0/25][369/782] Loss_D: 0.3477 Loss_G: 5.0168\n",
      "[0/25][370/782] Loss_D: 0.3115 Loss_G: 4.5391\n",
      "[0/25][371/782] Loss_D: 0.6425 Loss_G: 3.6376\n",
      "[0/25][372/782] Loss_D: 0.6644 Loss_G: 4.4375\n",
      "[0/25][373/782] Loss_D: 0.6241 Loss_G: 4.9519\n",
      "[0/25][374/782] Loss_D: 0.5161 Loss_G: 4.5157\n",
      "[0/25][375/782] Loss_D: 0.2800 Loss_G: 3.9999\n",
      "[0/25][376/782] Loss_D: 0.6000 Loss_G: 6.1756\n",
      "[0/25][377/782] Loss_D: 0.6713 Loss_G: 2.5319\n",
      "[0/25][378/782] Loss_D: 0.9721 Loss_G: 5.9434\n",
      "[0/25][379/782] Loss_D: 0.9976 Loss_G: 3.7617\n",
      "[0/25][380/782] Loss_D: 0.5249 Loss_G: 4.3384\n",
      "[0/25][381/782] Loss_D: 0.7867 Loss_G: 3.8169\n",
      "[0/25][382/782] Loss_D: 0.4517 Loss_G: 4.3493\n",
      "[0/25][383/782] Loss_D: 0.8834 Loss_G: 1.7551\n",
      "[0/25][384/782] Loss_D: 1.1164 Loss_G: 9.3567\n",
      "[0/25][385/782] Loss_D: 3.2256 Loss_G: 3.0796\n",
      "[0/25][386/782] Loss_D: 0.3551 Loss_G: 3.5635\n",
      "[0/25][387/782] Loss_D: 0.6522 Loss_G: 6.5173\n",
      "[0/25][388/782] Loss_D: 1.0323 Loss_G: 2.4962\n",
      "[0/25][389/782] Loss_D: 0.9129 Loss_G: 4.0560\n",
      "[0/25][390/782] Loss_D: 0.8791 Loss_G: 2.8776\n",
      "[0/25][391/782] Loss_D: 0.6745 Loss_G: 3.8987\n",
      "[0/25][392/782] Loss_D: 0.9931 Loss_G: 2.9978\n",
      "[0/25][393/782] Loss_D: 0.7660 Loss_G: 2.6657\n",
      "[0/25][394/782] Loss_D: 0.8364 Loss_G: 3.7885\n",
      "[0/25][395/782] Loss_D: 0.8194 Loss_G: 2.3306\n",
      "[0/25][396/782] Loss_D: 0.7503 Loss_G: 2.5843\n",
      "[0/25][397/782] Loss_D: 0.5270 Loss_G: 4.8114\n",
      "[0/25][398/782] Loss_D: 0.2757 Loss_G: 4.4239\n",
      "[0/25][399/782] Loss_D: 0.3043 Loss_G: 2.9216\n",
      "[0/25][400/782] Loss_D: 0.4517 Loss_G: 4.5466\n",
      "[0/25][401/782] Loss_D: 0.7374 Loss_G: 1.9042\n",
      "[0/25][402/782] Loss_D: 0.7340 Loss_G: 4.6467\n",
      "[0/25][403/782] Loss_D: 0.3348 Loss_G: 4.3780\n",
      "[0/25][404/782] Loss_D: 0.4794 Loss_G: 3.4852\n",
      "[0/25][405/782] Loss_D: 0.6584 Loss_G: 3.0106\n",
      "[0/25][406/782] Loss_D: 0.7758 Loss_G: 5.6344\n",
      "[0/25][407/782] Loss_D: 1.1862 Loss_G: 2.0085\n",
      "[0/25][408/782] Loss_D: 1.5306 Loss_G: 6.7418\n",
      "[0/25][409/782] Loss_D: 1.0952 Loss_G: 4.4131\n",
      "[0/25][410/782] Loss_D: 0.4822 Loss_G: 2.8137\n",
      "[0/25][411/782] Loss_D: 0.5444 Loss_G: 4.6216\n",
      "[0/25][412/782] Loss_D: 0.5899 Loss_G: 3.4114\n",
      "[0/25][413/782] Loss_D: 0.5137 Loss_G: 3.6296\n",
      "[0/25][414/782] Loss_D: 0.7545 Loss_G: 3.8416\n",
      "[0/25][415/782] Loss_D: 0.8266 Loss_G: 3.3634\n",
      "[0/25][416/782] Loss_D: 0.5705 Loss_G: 4.3842\n",
      "[0/25][417/782] Loss_D: 0.4006 Loss_G: 3.6358\n",
      "[0/25][418/782] Loss_D: 0.2956 Loss_G: 4.7654\n",
      "[0/25][419/782] Loss_D: 0.2881 Loss_G: 4.2762\n",
      "[0/25][420/782] Loss_D: 0.7058 Loss_G: 2.7106\n",
      "[0/25][421/782] Loss_D: 0.5656 Loss_G: 6.4521\n",
      "[0/25][422/782] Loss_D: 1.4552 Loss_G: 2.1732\n",
      "[0/25][423/782] Loss_D: 0.9233 Loss_G: 5.9956\n",
      "[0/25][424/782] Loss_D: 1.6043 Loss_G: 1.0819\n",
      "[0/25][425/782] Loss_D: 1.1045 Loss_G: 5.1320\n",
      "[0/25][426/782] Loss_D: 0.5080 Loss_G: 3.7312\n",
      "[0/25][427/782] Loss_D: 0.4230 Loss_G: 2.8475\n",
      "[0/25][428/782] Loss_D: 0.5229 Loss_G: 5.4387\n",
      "[0/25][429/782] Loss_D: 0.9913 Loss_G: 2.0766\n",
      "[0/25][430/782] Loss_D: 1.0134 Loss_G: 5.0899\n",
      "[0/25][431/782] Loss_D: 0.6574 Loss_G: 2.6917\n",
      "[0/25][432/782] Loss_D: 0.3881 Loss_G: 4.3110\n",
      "[0/25][433/782] Loss_D: 0.2030 Loss_G: 4.9043\n",
      "[0/25][434/782] Loss_D: 0.6405 Loss_G: 1.8092\n",
      "[0/25][435/782] Loss_D: 0.7743 Loss_G: 6.3589\n",
      "[0/25][436/782] Loss_D: 0.2852 Loss_G: 5.2017\n",
      "[0/25][437/782] Loss_D: 0.3253 Loss_G: 2.4251\n",
      "[0/25][438/782] Loss_D: 0.6508 Loss_G: 6.0682\n",
      "[0/25][439/782] Loss_D: 0.4755 Loss_G: 4.6644\n",
      "[0/25][440/782] Loss_D: 0.2354 Loss_G: 3.2596\n",
      "[0/25][441/782] Loss_D: 0.3048 Loss_G: 4.1872\n",
      "[0/25][442/782] Loss_D: 0.4682 Loss_G: 2.4846\n",
      "[0/25][443/782] Loss_D: 0.5669 Loss_G: 4.3985\n",
      "[0/25][444/782] Loss_D: 0.1988 Loss_G: 4.8533\n",
      "[0/25][445/782] Loss_D: 0.5071 Loss_G: 1.7163\n",
      "[0/25][446/782] Loss_D: 1.2047 Loss_G: 4.5536\n",
      "[0/25][447/782] Loss_D: 0.5925 Loss_G: 1.0201\n",
      "[0/25][448/782] Loss_D: 1.5446 Loss_G: 8.5244\n",
      "[0/25][449/782] Loss_D: 4.0200 Loss_G: 1.3672\n",
      "[0/25][450/782] Loss_D: 1.2961 Loss_G: 4.3804\n",
      "[0/25][451/782] Loss_D: 1.3258 Loss_G: 1.3574\n",
      "[0/25][452/782] Loss_D: 1.7256 Loss_G: 4.3292\n",
      "[0/25][453/782] Loss_D: 1.3554 Loss_G: 1.7135\n",
      "[0/25][454/782] Loss_D: 1.0975 Loss_G: 3.8450\n",
      "[0/25][455/782] Loss_D: 0.6906 Loss_G: 2.9238\n",
      "[0/25][456/782] Loss_D: 0.7209 Loss_G: 4.4975\n",
      "[0/25][457/782] Loss_D: 0.7093 Loss_G: 2.6748\n",
      "[0/25][458/782] Loss_D: 0.7318 Loss_G: 4.0795\n",
      "[0/25][459/782] Loss_D: 0.4520 Loss_G: 3.4490\n",
      "[0/25][460/782] Loss_D: 0.6810 Loss_G: 3.6677\n",
      "[0/25][461/782] Loss_D: 0.5345 Loss_G: 3.0599\n",
      "[0/25][462/782] Loss_D: 0.7339 Loss_G: 3.5719\n",
      "[0/25][463/782] Loss_D: 0.3629 Loss_G: 4.7702\n",
      "[0/25][464/782] Loss_D: 0.2959 Loss_G: 3.8993\n",
      "[0/25][465/782] Loss_D: 0.4427 Loss_G: 4.4263\n",
      "[0/25][466/782] Loss_D: 0.4229 Loss_G: 3.6791\n",
      "[0/25][467/782] Loss_D: 0.4805 Loss_G: 5.5541\n",
      "[0/25][468/782] Loss_D: 0.3959 Loss_G: 3.6554\n",
      "[0/25][469/782] Loss_D: 0.4490 Loss_G: 5.2464\n",
      "[0/25][470/782] Loss_D: 0.2881 Loss_G: 4.6241\n",
      "[0/25][471/782] Loss_D: 0.3206 Loss_G: 2.7123\n",
      "[0/25][472/782] Loss_D: 0.9301 Loss_G: 8.4025\n",
      "[0/25][473/782] Loss_D: 2.1796 Loss_G: 1.3220\n",
      "[0/25][474/782] Loss_D: 1.6156 Loss_G: 7.3769\n",
      "[0/25][475/782] Loss_D: 1.8370 Loss_G: 3.2126\n",
      "[0/25][476/782] Loss_D: 0.4622 Loss_G: 3.1923\n",
      "[0/25][477/782] Loss_D: 0.7472 Loss_G: 4.8363\n",
      "[0/25][478/782] Loss_D: 0.7775 Loss_G: 2.5651\n",
      "[0/25][479/782] Loss_D: 0.9299 Loss_G: 2.9016\n",
      "[0/25][480/782] Loss_D: 0.7411 Loss_G: 4.5669\n",
      "[0/25][481/782] Loss_D: 0.4288 Loss_G: 3.0577\n",
      "[0/25][482/782] Loss_D: 1.0234 Loss_G: 3.3204\n",
      "[0/25][483/782] Loss_D: 0.9209 Loss_G: 4.2417\n",
      "[0/25][484/782] Loss_D: 0.5981 Loss_G: 2.2660\n",
      "[0/25][485/782] Loss_D: 0.5529 Loss_G: 4.3735\n",
      "[0/25][486/782] Loss_D: 0.3503 Loss_G: 4.0155\n",
      "[0/25][487/782] Loss_D: 0.3922 Loss_G: 2.8265\n",
      "[0/25][488/782] Loss_D: 0.5424 Loss_G: 4.7605\n",
      "[0/25][489/782] Loss_D: 0.3837 Loss_G: 3.8054\n",
      "[0/25][490/782] Loss_D: 0.3856 Loss_G: 4.6751\n",
      "[0/25][491/782] Loss_D: 0.5819 Loss_G: 1.9265\n",
      "[0/25][492/782] Loss_D: 1.1515 Loss_G: 8.6632\n",
      "[0/25][493/782] Loss_D: 1.4444 Loss_G: 4.8993\n",
      "[0/25][494/782] Loss_D: 0.1951 Loss_G: 3.5653\n",
      "[0/25][495/782] Loss_D: 0.9112 Loss_G: 8.3379\n",
      "[0/25][496/782] Loss_D: 1.4844 Loss_G: 3.4269\n",
      "[0/25][497/782] Loss_D: 1.0539 Loss_G: 6.2408\n",
      "[0/25][498/782] Loss_D: 0.9711 Loss_G: 3.3957\n",
      "[0/25][499/782] Loss_D: 0.6426 Loss_G: 4.4353\n",
      "[0/25][500/782] Loss_D: 0.4562 Loss_G: 4.6487\n",
      "[0/25][501/782] Loss_D: 0.6638 Loss_G: 3.3867\n",
      "[0/25][502/782] Loss_D: 0.6146 Loss_G: 4.5409\n",
      "[0/25][503/782] Loss_D: 0.6715 Loss_G: 3.0622\n",
      "[0/25][504/782] Loss_D: 0.9653 Loss_G: 4.7705\n",
      "[0/25][505/782] Loss_D: 0.6360 Loss_G: 3.3426\n",
      "[0/25][506/782] Loss_D: 0.8774 Loss_G: 4.3006\n",
      "[0/25][507/782] Loss_D: 0.4471 Loss_G: 2.9175\n",
      "[0/25][508/782] Loss_D: 0.7449 Loss_G: 6.0692\n",
      "[0/25][509/782] Loss_D: 0.8265 Loss_G: 2.7471\n",
      "[0/25][510/782] Loss_D: 0.5966 Loss_G: 4.3091\n",
      "[0/25][511/782] Loss_D: 0.2975 Loss_G: 4.1630\n",
      "[0/25][512/782] Loss_D: 0.3367 Loss_G: 4.3872\n",
      "[0/25][513/782] Loss_D: 0.3149 Loss_G: 4.1961\n",
      "[0/25][514/782] Loss_D: 0.5854 Loss_G: 2.1501\n",
      "[0/25][515/782] Loss_D: 1.0718 Loss_G: 9.3642\n",
      "[0/25][516/782] Loss_D: 1.8340 Loss_G: 4.9034\n",
      "[0/25][517/782] Loss_D: 0.2265 Loss_G: 2.8907\n",
      "[0/25][518/782] Loss_D: 0.3693 Loss_G: 5.7897\n",
      "[0/25][519/782] Loss_D: 0.2603 Loss_G: 4.4698\n",
      "[0/25][520/782] Loss_D: 0.2301 Loss_G: 3.7407\n",
      "[0/25][521/782] Loss_D: 0.4094 Loss_G: 5.8757\n",
      "[0/25][522/782] Loss_D: 0.5834 Loss_G: 2.8305\n",
      "[0/25][523/782] Loss_D: 0.6586 Loss_G: 6.9644\n",
      "[0/25][524/782] Loss_D: 0.3312 Loss_G: 5.7387\n",
      "[0/25][525/782] Loss_D: 0.2753 Loss_G: 3.6042\n",
      "[0/25][526/782] Loss_D: 0.3718 Loss_G: 5.4072\n",
      "[0/25][527/782] Loss_D: 0.5776 Loss_G: 3.4979\n",
      "[0/25][528/782] Loss_D: 0.4390 Loss_G: 5.1254\n",
      "[0/25][529/782] Loss_D: 0.2114 Loss_G: 5.3194\n",
      "[0/25][530/782] Loss_D: 0.3641 Loss_G: 3.6246\n",
      "[0/25][531/782] Loss_D: 0.4927 Loss_G: 4.4581\n",
      "[0/25][532/782] Loss_D: 0.1867 Loss_G: 4.9606\n",
      "[0/25][533/782] Loss_D: 0.3055 Loss_G: 3.9983\n",
      "[0/25][534/782] Loss_D: 0.3338 Loss_G: 4.4940\n",
      "[0/25][535/782] Loss_D: 0.3751 Loss_G: 4.0410\n",
      "[0/25][536/782] Loss_D: 0.4611 Loss_G: 5.3233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][537/782] Loss_D: 0.3482 Loss_G: 4.2525\n",
      "[0/25][538/782] Loss_D: 0.7982 Loss_G: 4.6138\n",
      "[0/25][539/782] Loss_D: 0.3960 Loss_G: 4.6389\n",
      "[0/25][540/782] Loss_D: 0.5026 Loss_G: 3.3169\n",
      "[0/25][541/782] Loss_D: 0.8977 Loss_G: 7.5829\n",
      "[0/25][542/782] Loss_D: 0.8228 Loss_G: 3.9651\n",
      "[0/25][543/782] Loss_D: 0.7087 Loss_G: 4.9369\n",
      "[0/25][544/782] Loss_D: 0.2834 Loss_G: 5.2243\n",
      "[0/25][545/782] Loss_D: 0.2205 Loss_G: 4.4281\n",
      "[0/25][546/782] Loss_D: 0.3612 Loss_G: 4.6537\n",
      "[0/25][547/782] Loss_D: 0.3053 Loss_G: 3.6731\n",
      "[0/25][548/782] Loss_D: 0.6726 Loss_G: 5.9310\n",
      "[0/25][549/782] Loss_D: 0.6622 Loss_G: 2.0879\n",
      "[0/25][550/782] Loss_D: 1.5061 Loss_G: 11.0822\n",
      "[0/25][551/782] Loss_D: 2.3246 Loss_G: 5.7593\n",
      "[0/25][552/782] Loss_D: 0.2988 Loss_G: 2.5469\n",
      "[0/25][553/782] Loss_D: 1.3170 Loss_G: 8.0653\n",
      "[0/25][554/782] Loss_D: 1.5270 Loss_G: 3.1680\n",
      "[0/25][555/782] Loss_D: 1.0113 Loss_G: 4.0918\n",
      "[0/25][556/782] Loss_D: 0.8373 Loss_G: 3.7757\n",
      "[0/25][557/782] Loss_D: 0.7279 Loss_G: 4.9326\n",
      "[0/25][558/782] Loss_D: 1.0971 Loss_G: 2.7063\n",
      "[0/25][559/782] Loss_D: 0.7640 Loss_G: 5.0446\n",
      "[0/25][560/782] Loss_D: 0.9402 Loss_G: 1.5238\n",
      "[0/25][561/782] Loss_D: 1.1195 Loss_G: 7.2133\n",
      "[0/25][562/782] Loss_D: 1.1142 Loss_G: 3.8697\n",
      "[0/25][563/782] Loss_D: 0.2618 Loss_G: 2.6029\n",
      "[0/25][564/782] Loss_D: 0.8772 Loss_G: 5.5830\n",
      "[0/25][565/782] Loss_D: 1.0537 Loss_G: 1.6698\n",
      "[0/25][566/782] Loss_D: 0.8955 Loss_G: 5.4558\n",
      "[0/25][567/782] Loss_D: 0.8325 Loss_G: 2.4565\n",
      "[0/25][568/782] Loss_D: 0.7216 Loss_G: 2.9992\n",
      "[0/25][569/782] Loss_D: 0.5627 Loss_G: 2.7963\n",
      "[0/25][570/782] Loss_D: 0.6145 Loss_G: 4.2845\n",
      "[0/25][571/782] Loss_D: 0.6017 Loss_G: 2.7234\n",
      "[0/25][572/782] Loss_D: 0.3976 Loss_G: 2.6363\n",
      "[0/25][573/782] Loss_D: 0.5292 Loss_G: 3.6623\n",
      "[0/25][574/782] Loss_D: 0.6638 Loss_G: 1.9438\n",
      "[0/25][575/782] Loss_D: 0.7344 Loss_G: 4.5593\n",
      "[0/25][576/782] Loss_D: 0.3722 Loss_G: 3.9064\n",
      "[0/25][577/782] Loss_D: 1.0003 Loss_G: 1.3528\n",
      "[0/25][578/782] Loss_D: 1.1192 Loss_G: 5.3603\n",
      "[0/25][579/782] Loss_D: 1.0539 Loss_G: 2.7949\n",
      "[0/25][580/782] Loss_D: 0.6761 Loss_G: 3.6476\n",
      "[0/25][581/782] Loss_D: 0.4173 Loss_G: 3.7465\n",
      "[0/25][582/782] Loss_D: 0.5016 Loss_G: 3.6304\n",
      "[0/25][583/782] Loss_D: 0.5488 Loss_G: 3.5360\n",
      "[0/25][584/782] Loss_D: 0.5633 Loss_G: 4.0234\n",
      "[0/25][585/782] Loss_D: 0.5931 Loss_G: 2.1434\n",
      "[0/25][586/782] Loss_D: 0.7587 Loss_G: 6.2318\n",
      "[0/25][587/782] Loss_D: 1.0950 Loss_G: 2.3710\n",
      "[0/25][588/782] Loss_D: 0.6466 Loss_G: 4.2593\n",
      "[0/25][589/782] Loss_D: 0.3817 Loss_G: 4.1794\n",
      "[0/25][590/782] Loss_D: 0.3471 Loss_G: 3.1672\n",
      "[0/25][591/782] Loss_D: 0.3808 Loss_G: 4.5199\n",
      "[0/25][592/782] Loss_D: 0.5818 Loss_G: 2.4219\n",
      "[0/25][593/782] Loss_D: 0.6304 Loss_G: 4.9459\n",
      "[0/25][594/782] Loss_D: 0.6296 Loss_G: 2.6578\n",
      "[0/25][595/782] Loss_D: 0.7832 Loss_G: 5.0733\n",
      "[0/25][596/782] Loss_D: 0.6219 Loss_G: 1.9941\n",
      "[0/25][597/782] Loss_D: 1.2887 Loss_G: 5.4179\n",
      "[0/25][598/782] Loss_D: 1.1830 Loss_G: 1.3175\n",
      "[0/25][599/782] Loss_D: 1.2366 Loss_G: 4.2057\n",
      "[0/25][600/782] Loss_D: 0.7632 Loss_G: 1.8869\n",
      "[0/25][601/782] Loss_D: 1.2095 Loss_G: 6.4697\n",
      "[0/25][602/782] Loss_D: 1.3351 Loss_G: 2.3903\n",
      "[0/25][603/782] Loss_D: 0.8678 Loss_G: 3.2562\n",
      "[0/25][604/782] Loss_D: 0.6761 Loss_G: 4.1238\n",
      "[0/25][605/782] Loss_D: 0.5706 Loss_G: 2.9412\n",
      "[0/25][606/782] Loss_D: 0.8281 Loss_G: 3.9474\n",
      "[0/25][607/782] Loss_D: 0.5865 Loss_G: 2.8230\n",
      "[0/25][608/782] Loss_D: 0.4774 Loss_G: 3.2471\n",
      "[0/25][609/782] Loss_D: 0.4392 Loss_G: 3.5164\n",
      "[0/25][610/782] Loss_D: 0.3345 Loss_G: 3.4210\n",
      "[0/25][611/782] Loss_D: 0.3495 Loss_G: 3.7057\n",
      "[0/25][612/782] Loss_D: 0.4527 Loss_G: 2.6944\n",
      "[0/25][613/782] Loss_D: 0.3816 Loss_G: 4.8600\n",
      "[0/25][614/782] Loss_D: 0.5259 Loss_G: 2.3768\n",
      "[0/25][615/782] Loss_D: 0.4680 Loss_G: 4.7457\n",
      "[0/25][616/782] Loss_D: 0.2245 Loss_G: 4.1335\n",
      "[0/25][617/782] Loss_D: 0.2819 Loss_G: 2.7973\n",
      "[0/25][618/782] Loss_D: 0.6163 Loss_G: 4.9250\n",
      "[0/25][619/782] Loss_D: 0.8346 Loss_G: 2.0615\n",
      "[0/25][620/782] Loss_D: 0.6470 Loss_G: 4.0975\n",
      "[0/25][621/782] Loss_D: 0.5039 Loss_G: 4.6686\n",
      "[0/25][622/782] Loss_D: 0.8624 Loss_G: 0.9153\n",
      "[0/25][623/782] Loss_D: 1.5820 Loss_G: 4.5667\n",
      "[0/25][624/782] Loss_D: 0.6330 Loss_G: 1.3068\n",
      "[0/25][625/782] Loss_D: 1.2336 Loss_G: 8.1272\n",
      "[0/25][626/782] Loss_D: 2.7005 Loss_G: 2.2854\n",
      "[0/25][627/782] Loss_D: 0.6718 Loss_G: 2.6565\n",
      "[0/25][628/782] Loss_D: 0.8481 Loss_G: 5.2979\n",
      "[0/25][629/782] Loss_D: 1.1562 Loss_G: 1.1500\n",
      "[0/25][630/782] Loss_D: 1.3853 Loss_G: 5.3943\n",
      "[0/25][631/782] Loss_D: 0.9287 Loss_G: 2.9293\n",
      "[0/25][632/782] Loss_D: 0.8383 Loss_G: 2.1881\n",
      "[0/25][633/782] Loss_D: 1.0628 Loss_G: 3.8341\n",
      "[0/25][634/782] Loss_D: 0.9269 Loss_G: 2.2362\n",
      "[0/25][635/782] Loss_D: 0.7452 Loss_G: 3.1754\n",
      "[0/25][636/782] Loss_D: 0.6972 Loss_G: 3.1621\n",
      "[0/25][637/782] Loss_D: 0.4359 Loss_G: 3.6717\n",
      "[0/25][638/782] Loss_D: 0.7065 Loss_G: 1.8121\n",
      "[0/25][639/782] Loss_D: 0.8112 Loss_G: 5.8061\n",
      "[0/25][640/782] Loss_D: 0.8425 Loss_G: 3.1762\n",
      "[0/25][641/782] Loss_D: 0.3870 Loss_G: 2.9424\n",
      "[0/25][642/782] Loss_D: 0.4686 Loss_G: 3.4335\n",
      "[0/25][643/782] Loss_D: 0.3804 Loss_G: 3.7368\n",
      "[0/25][644/782] Loss_D: 0.3247 Loss_G: 3.4385\n",
      "[0/25][645/782] Loss_D: 0.3121 Loss_G: 3.4858\n",
      "[0/25][646/782] Loss_D: 0.4671 Loss_G: 3.7107\n",
      "[0/25][647/782] Loss_D: 0.2680 Loss_G: 4.1388\n",
      "[0/25][648/782] Loss_D: 0.4364 Loss_G: 3.0780\n",
      "[0/25][649/782] Loss_D: 0.7193 Loss_G: 4.7898\n",
      "[0/25][650/782] Loss_D: 0.4860 Loss_G: 3.8539\n",
      "[0/25][651/782] Loss_D: 0.4008 Loss_G: 3.9894\n",
      "[0/25][652/782] Loss_D: 0.5369 Loss_G: 3.5575\n",
      "[0/25][653/782] Loss_D: 0.5426 Loss_G: 5.5275\n",
      "[0/25][654/782] Loss_D: 0.5551 Loss_G: 3.4716\n",
      "[0/25][655/782] Loss_D: 0.5587 Loss_G: 5.5545\n",
      "[0/25][656/782] Loss_D: 0.2315 Loss_G: 4.8336\n",
      "[0/25][657/782] Loss_D: 0.3581 Loss_G: 3.6410\n",
      "[0/25][658/782] Loss_D: 0.4909 Loss_G: 6.3072\n",
      "[0/25][659/782] Loss_D: 0.4797 Loss_G: 3.0035\n",
      "[0/25][660/782] Loss_D: 0.8453 Loss_G: 6.5046\n",
      "[0/25][661/782] Loss_D: 0.8619 Loss_G: 1.1798\n",
      "[0/25][662/782] Loss_D: 1.5606 Loss_G: 8.7297\n",
      "[0/25][663/782] Loss_D: 2.2221 Loss_G: 2.3056\n",
      "[0/25][664/782] Loss_D: 0.9630 Loss_G: 5.4857\n",
      "[0/25][665/782] Loss_D: 0.6774 Loss_G: 3.0901\n",
      "[0/25][666/782] Loss_D: 0.9490 Loss_G: 4.7611\n",
      "[0/25][667/782] Loss_D: 0.8966 Loss_G: 1.9601\n",
      "[0/25][668/782] Loss_D: 0.9057 Loss_G: 5.6503\n",
      "[0/25][669/782] Loss_D: 0.5154 Loss_G: 3.7930\n",
      "[0/25][670/782] Loss_D: 0.5044 Loss_G: 2.4254\n",
      "[0/25][671/782] Loss_D: 1.0416 Loss_G: 4.6682\n",
      "[0/25][672/782] Loss_D: 0.8535 Loss_G: 2.1309\n",
      "[0/25][673/782] Loss_D: 0.6801 Loss_G: 3.6349\n",
      "[0/25][674/782] Loss_D: 0.5080 Loss_G: 4.7283\n",
      "[0/25][675/782] Loss_D: 0.4908 Loss_G: 3.1891\n",
      "[0/25][676/782] Loss_D: 0.6522 Loss_G: 3.8977\n",
      "[0/25][677/782] Loss_D: 0.4689 Loss_G: 5.4139\n",
      "[0/25][678/782] Loss_D: 0.7490 Loss_G: 2.5628\n",
      "[0/25][679/782] Loss_D: 0.8011 Loss_G: 4.3469\n",
      "[0/25][680/782] Loss_D: 0.5082 Loss_G: 5.1474\n",
      "[0/25][681/782] Loss_D: 0.4388 Loss_G: 3.4767\n",
      "[0/25][682/782] Loss_D: 0.2110 Loss_G: 4.0138\n",
      "[0/25][683/782] Loss_D: 0.5205 Loss_G: 5.8469\n",
      "[0/25][684/782] Loss_D: 0.5075 Loss_G: 3.4918\n",
      "[0/25][685/782] Loss_D: 0.4062 Loss_G: 5.5207\n",
      "[0/25][686/782] Loss_D: 0.5926 Loss_G: 3.0582\n",
      "[0/25][687/782] Loss_D: 0.4746 Loss_G: 5.1379\n",
      "[0/25][688/782] Loss_D: 0.7171 Loss_G: 2.4833\n",
      "[0/25][689/782] Loss_D: 0.9173 Loss_G: 7.3455\n",
      "[0/25][690/782] Loss_D: 1.5334 Loss_G: 2.1951\n",
      "[0/25][691/782] Loss_D: 1.0365 Loss_G: 7.3088\n",
      "[0/25][692/782] Loss_D: 1.1765 Loss_G: 1.5884\n",
      "[0/25][693/782] Loss_D: 1.1871 Loss_G: 7.4083\n",
      "[0/25][694/782] Loss_D: 0.7090 Loss_G: 4.6110\n",
      "[0/25][695/782] Loss_D: 0.4352 Loss_G: 2.3424\n",
      "[0/25][696/782] Loss_D: 0.9328 Loss_G: 4.8355\n",
      "[0/25][697/782] Loss_D: 0.4141 Loss_G: 4.8322\n",
      "[0/25][698/782] Loss_D: 1.0308 Loss_G: 1.7599\n",
      "[0/25][699/782] Loss_D: 1.5658 Loss_G: 6.6040\n",
      "[0/25][700/782] Loss_D: 1.6567 Loss_G: 1.5429\n",
      "[0/25][701/782] Loss_D: 1.0543 Loss_G: 7.0865\n",
      "[0/25][702/782] Loss_D: 0.9224 Loss_G: 4.3732\n",
      "[0/25][703/782] Loss_D: 0.2874 Loss_G: 3.1292\n",
      "[0/25][704/782] Loss_D: 0.5033 Loss_G: 4.9681\n",
      "[0/25][705/782] Loss_D: 0.3430 Loss_G: 4.5108\n",
      "[0/25][706/782] Loss_D: 0.6675 Loss_G: 4.3389\n",
      "[0/25][707/782] Loss_D: 0.5369 Loss_G: 3.3602\n",
      "[0/25][708/782] Loss_D: 0.6626 Loss_G: 3.8573\n",
      "[0/25][709/782] Loss_D: 0.4571 Loss_G: 3.8973\n",
      "[0/25][710/782] Loss_D: 0.6789 Loss_G: 2.9367\n",
      "[0/25][711/782] Loss_D: 0.4982 Loss_G: 4.4747\n",
      "[0/25][712/782] Loss_D: 0.4524 Loss_G: 4.1929\n",
      "[0/25][713/782] Loss_D: 0.6075 Loss_G: 3.6913\n",
      "[0/25][714/782] Loss_D: 0.4444 Loss_G: 4.0265\n",
      "[0/25][715/782] Loss_D: 0.6336 Loss_G: 3.6792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][716/782] Loss_D: 0.6036 Loss_G: 5.4812\n",
      "[0/25][717/782] Loss_D: 0.3767 Loss_G: 4.8490\n",
      "[0/25][718/782] Loss_D: 0.3772 Loss_G: 4.6572\n",
      "[0/25][719/782] Loss_D: 0.2629 Loss_G: 5.0160\n",
      "[0/25][720/782] Loss_D: 0.2936 Loss_G: 5.3786\n",
      "[0/25][721/782] Loss_D: 0.5308 Loss_G: 3.4389\n",
      "[0/25][722/782] Loss_D: 0.8192 Loss_G: 7.7601\n",
      "[0/25][723/782] Loss_D: 1.0202 Loss_G: 3.1165\n",
      "[0/25][724/782] Loss_D: 0.9392 Loss_G: 7.4238\n",
      "[0/25][725/782] Loss_D: 0.6265 Loss_G: 4.0182\n",
      "[0/25][726/782] Loss_D: 0.5746 Loss_G: 5.1075\n",
      "[0/25][727/782] Loss_D: 0.6791 Loss_G: 3.7579\n",
      "[0/25][728/782] Loss_D: 0.8760 Loss_G: 5.5110\n",
      "[0/25][729/782] Loss_D: 0.5532 Loss_G: 3.8451\n",
      "[0/25][730/782] Loss_D: 0.4146 Loss_G: 4.2394\n",
      "[0/25][731/782] Loss_D: 0.2692 Loss_G: 5.3350\n",
      "[0/25][732/782] Loss_D: 0.2852 Loss_G: 4.2246\n",
      "[0/25][733/782] Loss_D: 0.2923 Loss_G: 4.5628\n",
      "[0/25][734/782] Loss_D: 0.6999 Loss_G: 2.9415\n",
      "[0/25][735/782] Loss_D: 0.6004 Loss_G: 6.3733\n",
      "[0/25][736/782] Loss_D: 0.4279 Loss_G: 3.6214\n",
      "[0/25][737/782] Loss_D: 0.3899 Loss_G: 6.3383\n",
      "[0/25][738/782] Loss_D: 0.5903 Loss_G: 2.4710\n",
      "[0/25][739/782] Loss_D: 0.8247 Loss_G: 8.0782\n",
      "[0/25][740/782] Loss_D: 2.2149 Loss_G: 2.3755\n",
      "[0/25][741/782] Loss_D: 0.9853 Loss_G: 8.9092\n",
      "[0/25][742/782] Loss_D: 3.5387 Loss_G: 1.6703\n",
      "[0/25][743/782] Loss_D: 1.6836 Loss_G: 4.2312\n",
      "[0/25][744/782] Loss_D: 0.3599 Loss_G: 4.6944\n",
      "[0/25][745/782] Loss_D: 0.9701 Loss_G: 2.2026\n",
      "[0/25][746/782] Loss_D: 1.1004 Loss_G: 4.3443\n",
      "[0/25][747/782] Loss_D: 0.7876 Loss_G: 3.0010\n",
      "[0/25][748/782] Loss_D: 0.8083 Loss_G: 3.7970\n",
      "[0/25][749/782] Loss_D: 0.9563 Loss_G: 2.8341\n",
      "[0/25][750/782] Loss_D: 0.7472 Loss_G: 3.7240\n",
      "[0/25][751/782] Loss_D: 0.5907 Loss_G: 3.3992\n",
      "[0/25][752/782] Loss_D: 0.9368 Loss_G: 3.8843\n",
      "[0/25][753/782] Loss_D: 0.8413 Loss_G: 1.9693\n",
      "[0/25][754/782] Loss_D: 0.9467 Loss_G: 6.8606\n",
      "[0/25][755/782] Loss_D: 0.8559 Loss_G: 3.8666\n",
      "[0/25][756/782] Loss_D: 0.3091 Loss_G: 3.2168\n",
      "[0/25][757/782] Loss_D: 0.3885 Loss_G: 5.0466\n",
      "[0/25][758/782] Loss_D: 0.2914 Loss_G: 4.2944\n",
      "[0/25][759/782] Loss_D: 0.6246 Loss_G: 4.5186\n",
      "[0/25][760/782] Loss_D: 0.5319 Loss_G: 3.5487\n",
      "[0/25][761/782] Loss_D: 0.6798 Loss_G: 4.7978\n",
      "[0/25][762/782] Loss_D: 0.4993 Loss_G: 3.5529\n",
      "[0/25][763/782] Loss_D: 0.7136 Loss_G: 4.7361\n",
      "[0/25][764/782] Loss_D: 0.5532 Loss_G: 4.3001\n",
      "[0/25][765/782] Loss_D: 0.8745 Loss_G: 2.3284\n",
      "[0/25][766/782] Loss_D: 1.3286 Loss_G: 8.3143\n",
      "[0/25][767/782] Loss_D: 2.7789 Loss_G: 2.1696\n",
      "[0/25][768/782] Loss_D: 0.9286 Loss_G: 4.8022\n",
      "[0/25][769/782] Loss_D: 0.5761 Loss_G: 4.9169\n",
      "[0/25][770/782] Loss_D: 0.6699 Loss_G: 2.8574\n",
      "[0/25][771/782] Loss_D: 0.7958 Loss_G: 4.7431\n",
      "[0/25][772/782] Loss_D: 0.5174 Loss_G: 3.6131\n",
      "[0/25][773/782] Loss_D: 0.7589 Loss_G: 3.8794\n",
      "[0/25][774/782] Loss_D: 0.5148 Loss_G: 4.1414\n",
      "[0/25][775/782] Loss_D: 0.5989 Loss_G: 3.7693\n",
      "[0/25][776/782] Loss_D: 0.6360 Loss_G: 5.6685\n",
      "[0/25][777/782] Loss_D: 0.5125 Loss_G: 4.0690\n",
      "[0/25][778/782] Loss_D: 0.8329 Loss_G: 2.8598\n",
      "[0/25][779/782] Loss_D: 0.8117 Loss_G: 5.7518\n",
      "[0/25][780/782] Loss_D: 0.9005 Loss_G: 2.4459\n",
      "[0/25][781/782] Loss_D: 0.5293 Loss_G: 5.6310\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1): # We iterate over 25 epochs.\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0): # We iterate over the images of the dataset.\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad() # We initialize to 0 the gradients of the discriminator with respect to the weights.\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data # We get a real image of the dataset which will be used to train the discriminator.\n",
    "        input = Variable(real) # We wrap it in a variable.\n",
    "        target = Variable(torch.ones(input.size()[0])) # We get the target.\n",
    "        output = netD(input) # We forward propagate this real image into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errD_real = criterion(output, target) # We compute the loss between the predictions (output) and the target (equal to 1).\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) # We make a random input vector (noise) of the generator.\n",
    "        fake = netG(noise) # We forward propagate this random input vector into the neural network of the generator to get some fake generated images.\n",
    "        target = Variable(torch.zeros(input.size()[0])) # We get the target.\n",
    "        output = netD(fake.detach()) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errD_fake = criterion(output, target) # We compute the loss between the prediction (output) and the target (equal to 0).\n",
    "\n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake # We compute the total error of the discriminator.\n",
    "        errD.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the discriminator.\n",
    "        optimizerD.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the discriminator.\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad() # We initialize to 0 the gradients of the generator with respect to the weights.\n",
    "        target = Variable(torch.ones(input.size()[0])) # We get the target.\n",
    "        output = netD(fake) # We forward propagate the fake generated images into the neural network of the discriminator to get the prediction (a value between 0 and 1).\n",
    "        errG = criterion(output, target) # We compute the loss between the prediction (output between 0 and 1) and the target (equal to 1).\n",
    "        errG.backward() # We backpropagate the loss error by computing the gradients of the total error with respect to the weights of the generator.\n",
    "        optimizerG.step() # We apply the optimizer to update the weights according to how much they are responsible for the loss error of the generator.\n",
    "\n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.item(), errG.item())) # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).\n",
    "        if i % 100 == 0: # Every 100 steps:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True) # We save the real images of the minibatch.\n",
    "            fake = netG(noise) # We get our fake generated images.\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True) # We also save the fake generated images of the minibatch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
